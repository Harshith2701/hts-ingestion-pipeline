# ğŸ§¾ HTS Ingestion Pipeline

Hey! This is a personal project I built to learn how to set up a data pipeline using Docker, Airflow, and Postgres â€” and eventually explore and visualize some U.S. trade data in Power BI.

## ğŸ—ƒï¸ Whatâ€™s the data?

The data comes from the **U.S. International Trade Commission** â€” specifically the **Harmonized Tariff Schedule (HTS)** in JSON format.

Each entry is a tariff classification line. It includes:
- A unique code (`htsno`)
- A description of the product
- Its indentation level (how deeply nested the item is)
- General and special duty rates
- Some entries are just labeled "Other" â€” which I mark with a flag for filtering later

Basically, itâ€™s a giant list of how different goods are classified and taxed when imported.

---

## ğŸ§  Why I chose this dataset

Itâ€™s weirdly interesting. It shows how governments categorize literally everything from live horses to semiconductors.

Plus:
- Itâ€™s structured, but still needs cleaning
- It has real-world use (trade, tariffs, policy)
- I wanted to try processing something bigger and a bit messy

---

## ğŸ”§ What I built

This project does 3 main things:

1. **Ingests HTS JSON data from a public URL**
2. **Cleans it up and loads it into Postgres**
3. **Automates the whole thing using Airflow**

After that, I pulled the cleaned data into Power BI to explore it visually.

---

## ğŸ› ï¸ Tech stack

- **Python** for parsing and ingesting the data
- **Airflow** to automate the task on a schedule
- **PostgreSQL** to store the cleaned data
- **Docker** to containerize everything
- **Power BI** for dashboards

---

## ğŸ“¦ How the pipeline works

1. **Docker Compose** spins up:
   - A Postgres database
   - An Airflow container
   - An app container that runs the ingestion script

2. **Airflow DAG** triggers every 10 minutes (just for testing, can be changed)
3. It runs the Python function that:
   - Streams the large JSON
   - Cleans up text fields
   - Flags "other" items
   - Inserts it into the `hts_data` table in Postgres
4. I then use Power BI to connect to the DB and create visuals

---

## ğŸ“Š What I visualized in Power BI

- Count of HTS codes at each indent level (shows how detailed classification gets)
- Share of "Other" entries (which are vague catch-all lines)
- Most common general and special duty rates
- A searchable table of all items

It helped me spot which sections are bloated with vague "other" items, and where tariff rates cluster.

---

## ğŸ§¹ Cleaning choices I made

I dropped:
- `units`, `other`, and `footnotes` â€” they werenâ€™t adding much value
- Cleaned up extra whitespace
- Flagged entries that just said "Other" in the description

---

## ğŸ“ File overview

- `docker-compose.yml` â€“ brings everything up
- `my_ingest.py` â€“ core logic to stream + clean + insert
- `ingest_chunks.py` â€“ Airflow DAG that runs the job
- `Dockerfile` â€“ for the app container
- `requirements.txt` â€“ Python dependencies

---

## ğŸ’­ Final thoughts

This wasnâ€™t about building something perfect â€” I just wanted to get hands-on with Airflow, work with real JSON data, and hook it into Power BI. And now I know how to do that. Pretty cool.

---

